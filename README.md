
# Desafio BIX - Pipeline de Dados com AWS

Este projeto implementa um pipeline de dados utilizando serviÃ§os da AWS como Glue, Lambda, Redshift Serverless, S3 e Step Functions. O objetivo Ã© construir uma arquitetura robusta e modular para ingestÃ£o, processamento, e carga de dados em um data warehouse, com foco em boas prÃ¡ticas de engenharia de dados.

## ğŸ“ Arquitetura

- **Camada de ingestÃ£o**: Dados coletados via API, via DB Postgres e em formato Parquet (esses sÃ£o colocados manualmente no S3).
- **Camada Bronze/Silver**: Lambdas e Glue Jobs organizam e transformam os dados conforme a arquitetura Medallion.
- **PersistÃªncia final**: Dados transformados sÃ£o copiados para o Redshift Serverless.
- **OrquestraÃ§Ã£o**: Toda a cadeia Ã© executada por uma Step Function AWS.

## ğŸ—ºï¸ Diagramas da Arquitetura

Abaixo estÃ£o dois diagramas que ilustram a arquitetura da soluÃ§Ã£o:

### ğŸ”¹ VisÃ£o Geral da Arquitetura via Terraform

![Diagrama 1](utils/terraform.drawio.png)

### ğŸ”¹ Detalhamento da Pipeline

![Diagrama 2](utils/stepfunctions_graph.png)

## ğŸ§° Tecnologias e Componentes

- AWS Lambda (dividida em funÃ§Ãµes para API, bronze, silver, copy, etc.)
- AWS Glue (jobs de transformaÃ§Ã£o em PySpark)
- Amazon Redshift Serverless
- Amazon S3
- AWS Step Functions
- AWS CLI + Terraform
- Python, PySpark, Pandas
- Camada pÃºblica para dependÃªncias (`pandas`, `pyarrow`)
- Camada psycopg2 para conexÃ£o com banco e coleta dos dados

## ğŸ“ Estrutura

```
glue_jobs/              # Scripts PySpark para camada Bronze e Silver
lambda/                 # CÃ³digo das Lambdas organizadas por funÃ§Ã£o
layers/                 # DependÃªncias empacotadas como Lambda Layer
utils/                  # Scripts de build e automaÃ§Ã£o (ex: subir Glue Jobs)
terraform/              # Infraestrutura como cÃ³digo
```

## âœ… PrÃ©-requisitos

- AWS CLI configurada e autenticada (`aws configure`)
- Python 3.10+
- Terraform instalado (para provisionamento de infraestrutura)
- PermissÃµes adequadas para criar recursos (S3, IAM, Redshift, Glue, Lambda, etc.)

## ğŸš€ Como Executar

### 1. Build das Lambdas

Execute o script de build localizado em `utils/build_all.py` para empacotar todas as funÃ§Ãµes Lambda com suas dependÃªncias:

```bash
cd utils/
python build_all.py
```

### 2. Suba a infraestrutura

```bash
terraform init
terraform apply
```

### 3. FaÃ§a upload manual do dataset fornecido

Coloque o arquivo `categoria.parquet` fornecido no desafio em:

```
s3://<bucket>/raw/{data}/categoria.parquet
```

### 4. Suba os Glue Jobs

```bash
cd utils/
./upload_glue_jobs.sh  # ou execute os scripts individuais de build
```

### 5. Configure permissÃ£o manual no Redshift

No Console da AWS â†’ Redshift Serverless:

- VÃ¡ em **Workgroup** > **Permissions**
- Adicione a IAM Role usada pelas funÃ§Ãµes de COPY
- Marque a opÃ§Ã£o de acesso ao Redshift e ao S3

### 6. Recupere as credenciais do superusuÃ¡rio e crie as tabelas

- Acesse o **AWS Secrets Manager** e copie o usuÃ¡rio e senha do segredo do Redshift (`admin_bix` ou equivalente).
- Use essas credenciais para se conectar ao Query Editor v2.
- Execute os arquivos SQL de criaÃ§Ã£o de schema e tabelas que estÃ£o na pasta `utils/`:
  - `utils/redshift_tables.sql`

### 7. Execute a Lambda `lambda_db`

Essa Lambda executa os comandos SQL (como `TRUNCATE` e `COPY`) no Redshift. Para isso:

- A lambda irÃ¡ executar um `SELECT 1` e irÃ¡ falhar, porÃ©m irÃ¡ criar o usuÃ¡rio associado a role.
- Execute via Query Editor v2:

```sql
GRANT USAGE ON SCHEMA silver TO "IAMR:lambda-redshift-exec-role";
GRANT ALL ON ALL TABLES IN SCHEMA silver TO "IAMR:lambda-redshift-exec-role";
```

### 8. Execute a Step Function

ApÃ³s a infraestrutura e permissÃµes configuradas, inicie o fluxo principal via AWS Step Functions.

---

## âš ï¸ ObservaÃ§Ãµes Importantes

- âœ… **AWS CLI deve estar autenticada** com uma conta que tenha acesso completo aos serviÃ§os utilizados.
- ğŸ“ O dataset `categoria.parquet` **precisa ser carregado manualmente** no S3 antes de iniciar o pipeline.
- ğŸ§  A permissÃ£o de `COPY` no Redshift **nÃ£o Ã© automÃ¡tica** e exige configuraÃ§Ã£o no console.
- ğŸ” O Redshift sÃ³ reconhece a IAM Role como um usuÃ¡rio (`IAMR:...`) **apÃ³s a primeira execuÃ§Ã£o bem-sucedida** de uma Lambda com essa role.
- ğŸ§± Os Glue Jobs nÃ£o sÃ£o gerenciados diretamente pelo Terraform. Use os scripts de `utils/` para enviar os scripts `.py` para o S3.

---

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido por Pedro SÃ¡ para fins do desafio proposto.
